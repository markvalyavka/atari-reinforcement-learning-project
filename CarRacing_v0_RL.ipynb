{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarRacing game\n",
    "\n",
    "\n",
    "##### General description \n",
    "- It is one of the easiest **continuous control task** to learn from pixels, a top-down racing\n",
    "  environment.  \n",
    "- State consists of STATE_W x STATE_H pixels.\n",
    "- The episode finishes when all the tiles are visited. The car also can go\n",
    "  outside of the PLAYFIELD -  that is far off the track, then it will get -100\n",
    "  and die.\n",
    "- Some indicators are shown at the bottom of the window along with the state RGB\n",
    "  buffer. From left to right: \n",
    "  - the true speed \n",
    "  - four ABS sensors \n",
    "  - the steering wheel position \n",
    "  - gyroscope.\n",
    "  \n",
    "<img src=\"./res/car_racing_indicators.png\" width=\"400px\" height=\"500px\" />\n",
    "\n",
    "\n",
    "*Indicators example*\n",
    "\n",
    "\n",
    "##### Reward\n",
    "- The reward is -0.1 every frame and +1000/N for every track tile visited, where\n",
    "  N is the total number of tiles visited in the track. For example, if you have\n",
    "  finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "- The game is solved when the agent consistently gets 900+ points. The generated\n",
    "  track is random every episode.\n",
    "  \n",
    "\n",
    "*Taken from official **gym** game description [github repo](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py)*\n",
    "\n",
    "\n",
    "##### Rendered env example\n",
    "\n",
    "<img src=\"./res/car_racing_rendered_env.png\" width=\"400px\" height=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install necessary packages/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'stable-baselines3[extra]'\n",
    "!pip install tensorboard==1.15.0\n",
    "!pip install 'gym[box2d]' \n",
    "!pip install pyglet==1.5.11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create & Explore the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment(env_name):\n",
    "    return gym.make(env_name)\n",
    "    \n",
    "env = make_environment(\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space type -> Box([-1.  0.  0.], [1. 1. 1.], (3,), float32)\n",
      "Action space sample -> [-0.43569368  0.54446155  0.7904896 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space type -> {env.action_space}\")\n",
    "print(f\"Action space sample -> {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space type -> Box([[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]], [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]], (96, 96, 3), uint8)\n",
      "Observation space sample -> [[[ 83 118 169]\n",
      "  [ 86 148  34]\n",
      "  [ 30  65 116]\n",
      "  ...\n",
      "  [175 243 241]\n",
      "  [ 87  45  55]\n",
      "  [ 86 212  41]]\n",
      "\n",
      " [[206  14  44]\n",
      "  [219 210 168]\n",
      "  [ 62 245 181]\n",
      "  ...\n",
      "  [195  40 145]\n",
      "  [133  10 247]\n",
      "  [ 94  53 146]]\n",
      "\n",
      " [[ 31  51  59]\n",
      "  [  9 108   4]\n",
      "  [ 60 211  11]\n",
      "  ...\n",
      "  [206 117 197]\n",
      "  [132 117 240]\n",
      "  [172 150  41]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  2  28  57]\n",
      "  [115 196  75]\n",
      "  [177 109 102]\n",
      "  ...\n",
      "  [ 91  70 106]\n",
      "  [ 44 199  98]\n",
      "  [ 98 241   1]]\n",
      "\n",
      " [[165 157 101]\n",
      "  [ 56 207  27]\n",
      "  [214  77 228]\n",
      "  ...\n",
      "  [ 77 147 240]\n",
      "  [ 46 223 236]\n",
      "  [193 153  18]]\n",
      "\n",
      " [[149  70  28]\n",
      "  [ 96 149 184]\n",
      "  [ 15 171  27]\n",
      "  ...\n",
      "  [110   0  52]\n",
      "  [104 157 229]\n",
      "  [ 87 203  58]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation space type -> {env.observation_space}\")\n",
    "print(f\"Observation space sample -> {env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape -> (96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation space shape -> {env.observation_space.sample().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random dummy agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(0, EPISODES):\n",
    "    state = env.reset()\n",
    "    is_done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not is_done:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        # take a sample from action space (random action)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, is_done, additional_info = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Step -> {ep+1} | Score -> {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('car_racing_training', 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go with 800 points (\"solved threshold\")\n",
    "REWARD_THRESHOLD = 800\n",
    "\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback,\n",
    "    StopTrainingOnRewardThreshold\n",
    ")\n",
    "\n",
    "best_model_save_path = os.path.join('car_racing_training', 'trained_models')\n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=REWARD_THRESHOLD, \n",
    "                                             verbose=1)\n",
    "evaluation_callback = EvalCallback(env,\n",
    "                                  callback_on_new_best=stop_callback,\n",
    "                                  eval_freq=6000,\n",
    "                                  best_model_save_path=best_model_save_path,\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_trained_models_path = os.path.join('car_racing_training', 'trained_models', '300k_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_trained_models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(save_trained_models_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markiyanvalyavka/opt/anaconda3/envs/ai37/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1207..1513 -> 306-tiles track\n",
      "Track generation: 1255..1573 -> 318-tiles track\n",
      "Track generation: 1043..1313 -> 270-tiles track\n",
      "Track generation: 1320..1654 -> 334-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "Track generation: 1013..1271 -> 258-tiles track\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Track generation: 905..1139 -> 234-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "Track generation: 1118..1402 -> 284-tiles track\n",
      "Track generation: 1197..1508 -> 311-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(239.85585741400718, 149.00814733178092)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training took about 2.5 hrs for 300k steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How reward and episode length grew:\n",
    "\n",
    "<img src=\"./res/racing_results.png\" width=\"800px\" height=\"1000px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test env with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment(env_name):\n",
    "    return gym.make(env_name)\n",
    "    \n",
    "env = make_environment(\"CarRacing-v0\")\n",
    "\n",
    "for ep in range(0, 10):\n",
    "    state = env.reset()\n",
    "    is_done = False\n",
    "    score = 0 \n",
    "    print(state)\n",
    "    while not is_done:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        # take a sample from action space (random action)\n",
    "        action = model.predict(state)\n",
    "        state, reward, is_done, additional_info = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Step -> {ep+1} | Score -> {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Result with untrained / trained model\n",
    "\n",
    "#### Random agent\n",
    "<img src=\"./res/dummy_example.gif\" width=\"400px\" height=\"500px\" />\n",
    "\n",
    "#### Trained agent\n",
    "<img src=\"./res/trained_300k.gif\" width=\"400px\" height=\"500px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
